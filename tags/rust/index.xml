<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>虞双齐Golang开发与SRE运维</title>
    <link>https://yushuangqi.com/tags/rust.xml</link>
    <description>在 虞双齐Golang开发与SRE运维上关于的内容</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <managingEditor>devysq@gmail.com (虞双齐)</managingEditor>
    <webMaster>devysq@gmail.com (虞双齐)</webMaster>
    <atom:link href="/tags/rust.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TiDB架构的演进和开发哲学</title>
      <link>https://yushuangqi.com/blog/2017/tidb-jia-gou-de-yan-jin-he-kai-fa-zhe-xue.html</link>
      <pubDate>Fri, 17 Feb 2017 08:17:16 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2017/tidb-jia-gou-de-yan-jin-he-kai-fa-zhe-xue.html</guid>
      <description>https://segmentfault.com/a/
 本文来自 CSDN《程序员》2017 年 2 月的封面报道。
对于一个从零开始的数据库来说：选择什么语言，整体架构怎么做，要不要开源，如何去测试…太多的问题需要去考量。
 在本篇文章中，PingCAP 联合创始人兼 CTO 黄东旭对 TiDB 的开发历程进行了详细简介，为大家还原 TiDB 的架构演进全过程。
在大约两年前，我有一次做 MySQL 分库分表和中间件的经历，那时在中间件里做 sharding，把 16 个节点的 MySQL 扩到 32 节点，差不多要提前一个月做演练，再用一个礼拜来上线。我就在想，能不能有一个数据库可以让我们不再想分库分表这些东西？当时我们也刚刚做完 Codis，觉得分布式是个比较合适的解决方案。另外我一直在关注学术圈关于分布式数据库的最新进展，有看到谷歌在 2013 年发的 Spanner 和 F1 的论文，所以决定干脆就重新开始写一个数据库，从根本上解决 MySQL 扩展性的问题。
而决定之后发现面对的问题非常复杂：选择什么语言，整个架构怎么做，到底要不要开源……做基础软件有一个很重要的事情：写出来并不难，难的是你怎么保证这个东西写对了。尤其是对于业务方，他们所有的业务正确性是构建在基础软件的正确性上。所以，对于分布式系统来说，什么是写对了，怎么去测试，这都是很重要的问题。关于这些我想了很久。
一开始总是要起步的。当时就决定冷静一下，先确定一个目标：解决 MySQL 的问题。MySQL 是单机型数据库，它没有办法做全扩展，我们选择 MySQL 兼容，首先选择在协议和语法层面的兼容，因为已有的社区里边很多的海量的测试。第二点是用户的迁移成本，能让用户迁移得很顺畅。第三是因为万事开头难，必须得有一个明确的目标，选定一个目标去做，对开发人员来说心理的压力最小。确定目标以后，我们 3 个人的创始团队从原来的公司出来，拿了一笔比较大的风险投资，开始正式做这件事情。
兼容 MySQL 最简单的方案，就是直接用 MySQL。为了让这个东西尽快地做起来，我们一开始做了一个最简单的版本，复用 MySQL前端 代码，做一个分布式的存储引擎就可以了，这个事情想想还是蛮简单的，所以非常乐观，觉得这个战略很完美。
上图是我在 2015 年 4 月份用六个礼拜完成的第一个版本的框架，但是后来没好意思开源出来，虽然能跑，但是在性能上完全无法接受。我就想这个东西为什么这么慢？一步一步去看每一层，就想动手改，但是发现工程量巨大，比如 MySQL 的 SQL 优化器， 事务模型等等，完全没有办法下手。就像这个架构图里看到的，因为在 MySQL Engine 这一层，我们能做的事情太少了，所以就没有办法。
第一版实验到此宣告失败，现在看起来写 SQL parser 和优化器等这些已经是绕不开了，我们索性决定从头开始写，唯一给我安慰的就是终于可以使用我们最爱的编程语言了，就是 Go。
我们跟其他做这种软件的工程师的思路相反，选择了从上往下写，先写最顶层的 SQL 的接口 SQL Layer，我要保证这个东西长得跟 MySQL 一模一样，包括网络协议和语法层。从 TiDB 网络协议、SQL 的语法解析器、到 SQL 的优化器、执行器等基本从上到下写了一遍。这个阶段持续了大概三个月左右。从这个阶段开始，我们慢慢摸索出了几个实践中深有体会的开发哲学。</description>
    </item>
    
    <item>
      <title>Service层的是否必要性分析及案例</title>
      <link>https://yushuangqi.com/blog/2016/serviceceng-de-shi-fou-bi-yao-xing-fen-xi-ji-an-li.html</link>
      <pubDate>Sat, 31 Dec 2016 11:33:22 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/serviceceng-de-shi-fou-bi-yao-xing-fen-xi-ji-an-li.html</guid>
      <description>序言 此前，我看过这样的一个提问“Yii2框架中，有必要再分离service层么？”，从别人的回答中，自己也收获了答案，但我觉得还需要有个活生生的粟子，才具有更加清晰明了和强有力的说服力。如对我的实战经历感兴趣的继续往下看，喜欢的还可以点击推荐和收藏。在举粟子前，我先讲讲service是什么？有什么作用吧？免得还有人糊涂。
1、service是什么？
在面向OO的系统里，service就是biz manager，在面向过程的系统里service就是TS脚本。
2、service有什么作用？
service层的作用就是把这些需要多个model参与的复杂业务逻辑单独封装出来，这些model之间不再发生直接的依赖，而是在service层内协同完成逻辑。service层的第一个目的其实就是对model层进行解耦。
需求分析 1、在Yii2框架中建立service层，专门处理公共且复杂的业务逻辑。
效果图 1、在common下建立个service层。
2、部分公共数据处理逻辑（主要的数据处理都写在这里）。
代码分析 1、在commonservice下写个CluesBranchService.php文件，CluesBranchService类继承本模块主要的models类Chance。凡是关于Chance的公共业务逻辑都往这个文件里写。
namespace common\service; use Yii; use api\modules\v1\models\Sales; use api\modules\v1\chance\models\Chance; /** * //下属的线索公共数据处理逻辑 */ class CluesBranchService extends Chance { //下属的线索列表 public static function getIndex() { $SalesModel = new Sales(); $uids = $SalesModel-&amp;gt;sevenChild(Yii::$app-&amp;gt;user-&amp;gt;id); if(count($uids)){ $query = Chance::find()-&amp;gt;where([&#39;in&#39;,&#39;owner_id&#39;,$uids]); }else{ $query = Chance::find()-&amp;gt;where([&#39;owner_id&#39;=&amp;gt;&#39;-1&#39;]); } return $query; } }  2、Controllers里调用。
use common\service\CluesBranchService; $query = CluesBranchService::getIndex();  注释：这里返回的是\$query，而不是查询的结果，用过Yii2的都知道列表实现分页用的是ActiveDataProvider，不需要查出结果，为了统一起来所以这里直接返回\$query。如有特殊需要加where、andWhere或者获取数据结果的可以这样\$query-&amp;gt;where([&amp;lsquo;条件&amp;rsquo;]);\$query-&amp;gt;all()。
分析总结 以上是一个业务逻辑比较简单的service层的实现方式，看到这里可能还有人疑惑，到底应不应该分离service层？
简单粗暴的总结来说，如果你的某个业务逻辑，需要用到多个model，就放到service层里面去，如果只是这个model自己的事，跟其它的model没有任何关系，放到model里面就好。</description>
    </item>
    
    <item>
      <title>分布式系统测试那些事儿-理念</title>
      <link>https://yushuangqi.com/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--li-nian.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:42 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--li-nian.html</guid>
      <description>https://segmentfault.com/a/
本文整理自 PingCAP NewSQL Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。
文章较长，为方便大家阅读，会分为上中下三篇，本文为上篇。
今天主要是介绍分布式系统测试。对于 PingCAP 目前的现状来说，我们是觉得做好分布式系统测试比做一个分布式系统更难。就是你把它写出来不是最难的，把它测好才是最难的。大家肯定会觉得有这么夸张吗？那我们先从一个最简单的、每个人都会写的 Hello world 开始。
 A simple “Hello world” is a miracle
We should walk through all of the bugs in:
 Compiler
 Linker
 VM (maybe)
 OS
  其实这个 Hello world 能够每次都正确运行已经是一个奇迹了，为什么呢？首先，编译器得没 bug，链接器得没 bug ；然后我们可能跑在 VM 上，那 VM 还得没 bug；并且 Hello world 那还有一个 syscall，那我们还得保证操作系统没有 bug；到这还不算吧，我们还得要硬件没有 bug。所以一个最简单程序它能正常运行起来，我们要穿越巨长的一条路径，然后这个路径里面所有的东西都不能出问题，我们才能看到一个最简单的 Hello world。
 但是分布式系统里面呢，就更加复杂了。比如大家现在用的很典型的微服务。假设你提供了一个微服务，然后在微服务提供的功能就是输出一个 Hello world ，然后让别人来 Call。
 A RPC “Hello world” is a miracle</description>
    </item>
    
    <item>
      <title>分布式系统测试那些事儿-错误注入</title>
      <link>https://yushuangqi.com/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--cuo-wu-zhu-ru.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:38 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--cuo-wu-zhu-ru.html</guid>
      <description>本话题系列文章整理自 PingCAP NewSQL Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为中篇。
接上篇：
当然测试可能会让你代码变得没有那么漂亮，举个例子：
这是知名的 Kubernetes 的代码，就是说它有一个 DaemonSetcontroller，这 controller 里面注入了三个测试点，比如这个地方注入了一个 handler ，你可以认为所有的注入都是 interface。比如说你写一个简单的 1+1=2 的程序，假设我们写一个计算器，这个计算器的功能就是求和，那这就很难注入错误。所以你必须要在你正确的代码里面去注入测试逻辑。再比如别人 call 你的这个 add 的 function，然后你是不是有一个 error？这个 error 的问题是它可能永远不会返回一个 error，所以你必须要人肉的注进去，然后看应用程序是不是正确的行为。说完了加法，再说我们做一个除法。除法大家知道可能有处理异常，那上面是不是能正常处理呢？上面没有，上面写着一个比如说 6 ÷ 3，然后写了一个 test，coverage 100%，但是一个除零异常，系统就崩掉了，所以这时候就需要去注入错误。大名鼎鼎的 Kubernetes 为了测试各种异常逻辑也采用类似的方式，这个结构体不算长，大概是十几个成员，然后里面就注入了三个点，可以在里面注入错误。
那么在设计 TiDB 的时候，我们当时是怎么考虑 test 这个事情的？首先一个百万级的 test 不可能由人肉来写，也就是说你如果重新定义一个自己的所谓的 SQL 语法，或者一个 query language，那这个时候你需要构建百万级的 test，即使全公司去写，写个两年都不够，所以这个事情显然是不靠谱的。但是除非说我的 query language 特别简单，比如像 MongoDB 早期的那种，那我一个“大于多少”的这种，或者 equal 这种条件查询特别简单的，那你确实是不需要构建这种百万级的 test。但是如果做一个 SQL 的 database 的话，那是需要构建这种非常非常复杂的 test 的。这时候这个 test 又不能全公司的人写个两年，对吧？所以有什么好办法呢？MySQL 兼容的各种系统都是可以用来 test 的，所以我们当时兼容 MySQL 协议，那意味着我们能够取得大量的 MySQL test。不知道有没有人统计过 MySQL 有多少个 test，产品级的 test 很吓人的，千万级。然后还有很多 ORM， 支持 MySQL 的各种应用都有自己的测试。大家知道，每个语言都会 build 自己的 ORM，然后甚至是一个语言的 ORM 都有好几个。比如说对于 MySQL 可能有排第一的、排第二的，那我们可以把这些全拿过来用来测试我们的系统。</description>
    </item>
    
    <item>
      <title>分布式系统测试那些事儿-信心的毁灭与重建</title>
      <link>https://yushuangqi.com/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--xin-xin-de-hui-mie-yu-chong-jian.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:36 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--xin-xin-de-hui-mie-yu-chong-jian.html</guid>
      <description>本话题系列文章整理自 PingCAP NewSQL Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为下篇。
接中篇：
ScyllaDB 有一个开源的东西，是专门用来给文件系统做 Failure Injection 的, 名字叫做 CharybdeFS。如果你想测试你的系统，就是文件系统在哪不断出问题，比如说写磁盘失败了，驱动程序分配内存失败了，文件已经存在等等，它都可以测模拟出来。
 CharybdeFS: A new fault-injecting file system for software testing
Simulate the following errors:
 disk IO error (EIO)
 driver out of memory error (ENOMEM)
 file already exists (EEXIST)
 disk quota exceeded (EDQUOT)
  再来看看 Cloudera，下图是整个 Cloudera 的一个 Failure Injection 的结构。
 一边是 Tools，一边是它的整个的 Level 划分。比如说整个 Cluster， Cluster 上面有很多 Host，Host 上面又跑了各种 Service，整个系统主要用于测试 HDFS， HDFS 也是很努力的在做有效的测试。然后每个机器上部署一个 AgenTEST，就用来注射那些可能出现的错误。</description>
    </item>
    
    <item>
      <title>TiKV源码解析系列-multi-raft设计与实现</title>
      <link>https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--multi-raft-she-ji-yu-shi-xian.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:35 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--multi-raft-she-ji-yu-shi-xian.html</guid>
      <description>本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。
需要注意，TiKV 使用 Rust 语言编写，用户需要对 Rust 语言有一个大概的了解。另外，本系列文章并不会涉及到 TiKV 中心控制服务 Placement Driver(PD) 的详细介绍，但是会说明一些重要流程 TiKV 是如何与 PD 交互的。
TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。
本文为本系列文章第二节。
Placement Driver 在继续之前，我们先简单介绍一下 Placement Driver(PD)。PD 是 TiKV 的全局中央控制器，存储整个 TiKV 集群的元数据信息，负责整个 TiKV 集群的调度，全局 ID 的生成，以及全局 TSO 授时等。
PD 是一个非常重要的中心节点，它通过集成 etcd，自动的支持了分布式扩展以及 failover，解决了单点故障问题。关于 PD 的详细介绍，后续我们会新开一篇文章说明。
在 TiKV 里面，跟 PD 的交互是放在源码的 pd 目录下，现在跟 PD 的交互都是通过自己定义的 RPC 实现，协议非常简单，在 pd/mod.rs 里面我们直接提供了用于跟 PD 进行交互的 Client trait，以及实现了 RPC Client。</description>
    </item>
    
    <item>
      <title>TiKV源码解析系列-如何使用Raft</title>
      <link>https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--ru-he-shi-yong--raft.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:35 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--ru-he-shi-yong--raft.html</guid>
      <description>TiKV 源码解析系列——如何使用 Raft 本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。
需要注意，TiKV 使用 Rust 语言编写，用户需要对 Rust 语言有一个大概的了解。另外，本系列文章并不会涉及到 TiKV 中心控制服务 Placement Driver(PD) 的详细介绍，但是会说明一些重要流程 TiKV 是如何与 PD 交互的。
TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。
概述 本文档主要面向 TiKV 社区开发者，主要介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读文档之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。
需要注意，TiKV 使用 Rust 语言编写，用户需要对 Rust 语言有一个大概的了解。另外，本文档并不会涉及到 TiKV 中心控制服务 Placement Driver(PD) 的详细介绍，但是会说明一些重要流程 TiKV 是如何与 PD 交互的。
TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。</description>
    </item>
    
    <item>
      <title>TiKV源码解析系列-PlacementDriver</title>
      <link>https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie---placement-driver.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:32 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie---placement-driver.html</guid>
      <description>本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。
TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。
本文为本系列文章第三节。
介绍 Placement Driver (后续以 PD 简称) 是 TiDB 里面全局中心总控节点，它负责整个集群的调度，负责全局 ID 的生成，以及全局时间戳 TSO 的生成等。PD 还保存着整个集群 TiKV 的元信息，负责给 client 提供路由功能。
作为中心总控节点，PD 通过集成 etcd ，自动的支持 auto failover，无需担心单点故障问题。同时，PD 也通过 etcd 的 raft，保证了数据的强一致性，不用担心数据丢失的问题。
在架构上面，PD 所有的数据都是通过 TiKV 主动上报获知的。同时，PD 对整个 TiKV 集群的调度等操作，也只会在 TiKV 发送 heartbeat 命令的结果里面返回相关的命令，让 TiKV 自行去处理，而不是主动去给 TiKV 发命令。这样设计上面就非常简单，我们完全可以认为 PD 是一个无状态的服务（当然，PD 仍然会将一些信息持久化到 etcd），所有的操作都是被动触发，即使 PD 挂掉，新选出的 PD leader 也能立刻对外服务，无需考虑任何之前的中间状态。</description>
    </item>
    
    <item>
      <title>TiDB优化器实现的基础:统计信息的收集</title>
      <link>https://yushuangqi.com/blog/2016/tidb-you-hua-qi-shi-xian-de-ji-chu-tong-ji-xin-xi-de-shou-ji.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:31 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/tidb-you-hua-qi-shi-xian-de-ji-chu-tong-ji-xin-xi-de-shou-ji.html</guid>
      <description>https://segmentfault.com/a/
 收集统计信息的意义
一个 SQL 数据库里，优化器实现的好坏对性能的影响是决定性的。一个未经优化的执行计划和经过充分优化后的执行计划，执行时间的差别往往是成千上万倍。而对一个 SQL 优化器来说，统计信息是必不可少的条件，只有依赖统计信息提供的数据，优化器才可以正确估算不同的执行计划的执行代价，以选择最优的执行计划。就像一个大厨无论多么优秀，没有上等食材也是无法做出美味的饭菜。
 统计信息包含的内容 统计信息有两类，包括 Table 统计信息和 Column 统计信息。
Table 统计信息包含 Row Count 和 Row Size。
Column 统计信息包含 Null Count，Max Value，Min Value，Distinct Count 以及 Histogram。其中 Histogram 用来记录这个 Column 的数据详细分布，可以用来估算大于、小于或等于某个 Value 的 Row Count。
统计信息采集的步骤 1）在 TiDB 执行 ANALYZE TABLE 语句，手动触发收集动作。
我们知道，一个 Table 的数据往往是在不断变化的，我们无法保证统计信息时刻保持在最新状态，总会有一定的误差，如果我们不及时更新，随着时间的推进，这个误差可能会越来越大，影响到优化器的优化效果。
有时我们会需要让统计信息更新的频率低一些来降低系统的压力，因为每次的统计信息收集都是开销很大的操作。有时我们会需要立即更新统计数据，因为我们刚刚向一个表导入了大量的数据，马上就需要查询。
所以定期更新统计信息的功能，我们希望可以用独立的模块，用更灵活的策略来实现，TiDB 本身只需要支持基本的手动触发就可以了。
2）全表扫描。
全表扫描的执行过程比较长，整个扫表的任务会被分解为一系列 Region 的小请求，每个 Region 的请求会返回该 Region 包含的 Table 的部分数据。
3）用扫描得到的数据，记录 Row Count 和 Row Size，并对数据采样。
扫描得到的数据量有可能会非常大，以至于无法把全部数据保留在内存里进行处理，所以需要进行采样，当采样的概率均匀的时候，计算生成的统计信息的误差是可以接受的。这里我们设定的采样数是 1 万，无论 Table 有多大，最后保留的样本数不会超过 1 万。后面会详细介绍采样时使用到的算法。</description>
    </item>
    
    <item>
      <title>TiDB源码初探</title>
      <link>https://yushuangqi.com/blog/2016/tidb-yuan-ma-chu-tan.html</link>
      <pubDate>Sat, 31 Dec 2016 11:32:30 +0800</pubDate>
      <author>devysq@gmail.com (虞双齐)</author>
      <guid>https://yushuangqi.com/blog/2016/tidb-yuan-ma-chu-tan.html</guid>
      <description>作者: @申砾
本文档面向 TiDB 社区开发者，主要介绍 TiDB 的系统架构、代码结构以及执行流程。 目的是使得开发者阅读文档后，可以对 TiDB 项目有一个整体的了解，更好的参与进来。首先会介绍一下大体的结构以及 Golang 包的结构，然后会介绍内部的执行流程，最后会对优化器、执行器这两个最重要的组件做一些说明。
系统架构 TiDB Server 在整个系统中位于 Load Balancer(或者是 Application) 与底层的存储引擎之间，主要部分分为三层：
MySQL Protocol 层
接收 MySQL Client 的请求，解析 MySQL Protocol 的包并转换为 TiDB Session 中的各种命令；处理完成后，将结果结果为 MySQL Protocol 格式，返回给 Client。
SQL 层
解析并执行 SQL 语句，制定查询计划并优化，生成执行器并通过 KV 层读取或写入数据，最后返回结果给 MySQL Protocol层。这一层是重点，后面会详细介绍。
KV 层
提供带事务的（分布式/单机）存储，在 KV 层和 SQL 层之间，有一层抽象，使得 SQL 层能够忽略下面不同的 KV 存储的差异，看到统一的接口。
代码结构概述 这里首先会把所有的 package 列出来，然后介绍其主要功能。这一章会比较散，信息量比较大，可以结合下一章节一起理解。
tidb
这个包可以认为是 MySQL Protocol Layer 和 SQL Layer 之间的接口，主要的文件有三个：</description>
    </item>
    
  </channel>
</rss>